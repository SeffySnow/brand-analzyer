# Brand Analyzer - Comprehensive Documentation

A sophisticated brand analysis tool that uses LLM-powered web search to provide comprehensive brand insights with advanced tokenization optimization techniques.

## üéØ **Main Idea**

The Brand Analyzer is designed to answer questions about brands and companies by:
1. **Web Search Integration**: Using LLM to perform intelligent web searches
2. **Citation Extraction**: Automatically extracting and categorizing citations from responses
3. **Brand Mention Analysis**: Identifying and analyzing brand mentions in various formats
4. **Token Optimization**: Implementing advanced techniques to reduce token usage while maintaining quality
5. **Performance Evaluation**: Comprehensive testing and analysis of different approaches

## üèóÔ∏è **Project Architecture**

```
brand_analyzer/
‚îú‚îÄ‚îÄ main.py                           # Main application entry point
‚îú‚îÄ‚îÄ utils.py                          # Core utility functions
‚îú‚îÄ‚îÄ requirements.txt                  # Dependencies
‚îú‚îÄ‚îÄ .env                              # Environment configuration
‚îú‚îÄ‚îÄ evaluation/                       # Evaluation and analysis
‚îÇ   ‚îú‚îÄ‚îÄ                     # Tokenization analysis
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fresh_tokenization_evaluation.py  # Main evaluation class
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ token_optimization.py              # Optimization techniques
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scripts/                          # Executable scripts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ results/                          # Analysis results
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ docs/                            # Documentation
‚îÇ   ‚îî‚îÄ‚îÄ tests/                        # Test cases
‚îî‚îÄ‚îÄ cache/                           # Automatic caching system
```

## üìã **Core Components**

### 1. **main.py** - CLI Application

**Purpose**: Full-featured CLI tool for brand analysis

**Key Commands**:
- **`analyze`**: Main analysis command with comprehensive options
- **`stats`**: Show statistics from previous analyses
- **`config`**: Display current configuration and environment setup
- **`FIXED_PROMPT_PART`**: The core prompt template used for LLM interactions

**CLI Features**:
- **Multiple Commands**: `analyze`, `stats`, `config`
- **Rich Options**: Custom output files, verbose mode, parameter control
- **Progress Indicators**: Visual feedback during analysis
- **Statistics**: Analysis history and performance metrics
- **Configuration Check**: Environment validation
- **Help System**: Comprehensive help and examples

**Analysis Flow**:
1. Loads environment variables and configuration
2. Takes brand name, website URL, and question as input
3. Creates comprehensive prompt using `FIXED_PROMPT_PART`
4. Calls LLM to generate response with web search capabilities
5. Extracts and analyzes citations, mentions, and sources
6. Saves structured JSON results and displays analysis

**Key Features**:
- **Automatic Caching**: Responses are cached to avoid redundant API calls
- **Token Counting**: Tracks input/output token usage
- **Error Handling**: Graceful error handling with informative messages
- **Rich CLI**: Professional command-line interface with colors and progress bars
- **Statistics Tracking**: Maintains analysis history and performance metrics

### 2. **utils.py** - Core Utility Functions

**Purpose**: Contains all the core functionality for brand analysis

**Key Functions**:

#### **Token Management**:
- **`count_tokens(text, model)`**: Counts tokens using tiktoken encoding
- **`create_prompt(brand_name, website_url, question, max_searches, max_sources)`**: Creates structured prompts

#### **LLM Integration**:
- **`generate_llm_response(prompt, brand_name, max_searches, max_sources)`**: 
  - Connects to OpenRouter API
  - Uses `nousresearch/hermes-2-pro-llama-3-8b` model
  - Handles API errors and timeouts
  - Returns raw LLM response

#### **Content Analysis**:
- **`extract_citations(response_text)`**: 
  - Extracts markdown links `[text](url)` from responses
  - Returns structured citation data with positions and context
  - Handles various citation formats

- **`extract_mentions(response_text, brand_name)`**: 
  - Identifies brand mentions in multiple formats:
    - `[Brand](url)` - markdown links
    - `Brand[url]` - inline links  
    - `Brand` - standalone mentions
  - **Ignores URLs**: Excludes brand mentions found within URLs (e.g., "tesla.com")
  - Returns detailed mention analysis with types and positions

- **`classify_sources(citations)`**: 
  - Categorizes sources as "owned" (brand's domain) or "external"
  - Uses URL parsing to determine ownership

#### **Response Processing**:
- **`parse_search_usage_from_response(response_text)`**: 
  - Extracts search usage information from LLM responses
  - Parses search count and remaining searches

**Advanced Features**:
- **URL Validation**: Ensures extracted URLs are valid
- **Context Extraction**: Provides surrounding text for citations/mentions
- **Position Tracking**: Records exact positions of extracted elements
- **Error Handling**: Robust error handling for all functions

### 3. **token_optimization.py** - Advanced Optimization Techniques

**Purpose**: Implements sophisticated token reduction techniques while maintaining response quality

**Key Classes**:

#### **PerformanceMonitor**:
- Tracks execution times for different operations
- Provides performance metrics and averages
- Logs timing information for analysis

#### **CacheManager**:
- Implements intelligent caching system
- Uses content hashing for cache keys
- Manages cache storage and retrieval
- Handles cache expiration and cleanup

#### **TokenOptimizer** (Main Class):
**Core Optimization Techniques**:

1. **BPE Optimization** (`optimize_with_bpe()`):
   - Uses Byte Pair Encoding for token reduction
   - Identifies common patterns and replaces them with shorter tokens
   - Maintains semantic meaning while reducing token count

2. **Semantic Compression** (`optimize_with_semantic_compression()`):
   - **Abbreviation System**: Replaces common phrases with abbreviations
   - **Synonym Replacement**: Uses shorter synonyms for common words
   - **Pattern Compression**: Compresses repetitive patterns
   - **Context Preservation**: Maintains meaning while reducing tokens

3. **Hierarchical Compression** (`optimize_with_hierarchical_compression()`):
   - **Multi-level Compression**: Applies compression at different levels
   - **Progressive Optimization**: Gradually reduces tokens while monitoring quality
   - **Adaptive Thresholds**: Adjusts compression based on content type

**Advanced Features**:
- **Quality Monitoring**: Tracks quality metrics during optimization
- **Adaptive Optimization**: Adjusts techniques based on content
- **Performance Tracking**: Monitors optimization performance
- **Cache Integration**: Uses caching to avoid redundant processing

### 4. **fresh_tokenization_evaluation.py** - Comprehensive Evaluation System

**Purpose**: Evaluates different tokenization techniques against real-world performance metrics

**Key Components**:

#### **FreshTokenizationEvaluation Class**:

**Test Cases**:
- **Tesla Short/Long**: Different complexity levels for Tesla analysis
- **Google Short/Long**: Different complexity levels for Google analysis
- **Varied Questions**: Different types of brand-related questions

**Evaluation Techniques**:
1. **Main_Py_Standard**: Uses the actual `FIXED_PROMPT_PART` from main.py
2. **BPE_Optimized**: Applies BPE optimization to the standard prompt
3. **Semantic_Compression**: Uses semantic compression techniques
4. **Hierarchical_Compression**: Applies hierarchical compression

**Key Methods**:

- **`run_comprehensive_evaluation()`**: 
  - Runs all test cases with all techniques
  - Collects detailed performance metrics
  - Handles errors gracefully
  - Returns comprehensive results

- **`_run_standard()`**: 
  - Uses main.py's exact prompt structure
  - Maintains same LLM configuration
  - Provides baseline performance metrics

- **`_run_with_optimization()`**: 
  - Applies token optimization techniques
  - Measures optimization impact
  - Tracks quality vs. efficiency trade-offs

**Metrics Collected**:
- **Token Usage**: Input, output, and total tokens
- **Response Time**: Execution duration
- **Content Quality**: Citations and mentions count
- **Source Classification**: Owned vs. external sources
- **Error Rates**: Success/failure rates

## üî¨ **Evaluation Results**

### **Token Optimization Performance Analysis**

Based on comprehensive testing across different question lengths and brands:

| Technique | Short Questions | Long Questions | Best Use Case |
|-----------|----------------|----------------|---------------|
| **Original Prompt** | Baseline (0% optimization) | Baseline (0% optimization) | Reference standard |
| **Optimized Prompt** | ‚úÖ **30% token reduction** | ‚úÖ **20% token reduction** | **Short questions** |
| **Compressed Prompt** | ‚ùå **-8% token increase** | ‚úÖ **30% token reduction** | **Long questions** |

### **Key Performance Insights**

#### **Short Questions Performance**:
- **Optimized Prompt**: Best choice for short questions
  - 30.16% token reduction (Apple)
  - 29.92% token reduction (Google)
  - ‚úÖ Maintains budget compliance
  - ‚úÖ Consistent performance across brands

- **Compressed Prompt**: Poor performance on short questions
  - -10.32% token increase (Apple)
  - -6.3% token increase (Google)
  - ‚ùå Budget violations
  - ‚ùå Over-compression leads to inefficiency

#### **Long Questions Performance**:
- **Compressed Prompt**: Best choice for long questions
  - 29.38% token reduction (Apple)
  - 29.74% token reduction (Google)
  - ‚úÖ Highest token savings
  - ‚úÖ Effective compression of complex content

- **Optimized Prompt**: Moderate performance on long questions
  - 19.59% token reduction (Apple)
  - 19.49% token reduction (Google)
  - ‚ö†Ô∏è Some budget violations
  - ‚ö†Ô∏è Less effective on complex content

### **Strategic Recommendations**

1. **Use Optimized Prompt for Short Questions**: Achieves 30% token reduction with reliable budget compliance
2. **Use Compressed Prompt for Long Questions**: Achieves 30% token reduction on complex content
3. **Avoid Compressed Prompt for Short Questions**: Causes token increase and budget violations
4. **Consider Original Prompt**: When quality is paramount and token efficiency is secondary

## üîß **Technical Implementation Details**

### **Assumptions and Design Decisions**

#### **Core Assumptions**:
1. **Web Search Capability**: LLM can perform intelligent web searches when prompted appropriately
2. **Citation Format**: Standard markdown link format `[text](url)` is the most reliable citation method
3. **Token Efficiency**: Shorter prompts with clear constraints lead to better budget compliance
4. **Brand Recognition**: Brand mentions can be reliably detected through text pattern matching
5. **Source Classification**: Domain-based classification accurately distinguishes owned vs external sources

#### **Design Philosophy**:
- **Constraint-First**: Hard limits on searches and sources prevent runaway costs
- **Quality Preservation**: Maintains answer quality while optimizing token usage
- **Transparency**: All citations and sources are clearly marked and categorized
- **Robustness**: Graceful degradation when external services fail

### **Answer Fidelity Preservation**

#### **How We Maintain "Average ChatGPT User" Quality**:

1. **Prompt Engineering**:
   - Clear, direct instructions that mirror how users typically ask questions
   - Explicit formatting requirements for citations
   - Constraint communication that doesn't compromise answer quality

2. **Citation Requirements**:
   - Mandatory source citations for all claims
   - Inline citation format `[text](url)` for easy verification
   - Source diversity requirements (owned + external sources)

3. **Content Structure**:
   - Plain markdown format for readability
   - Logical flow from question to answer
   - Comprehensive coverage without unnecessary verbosity

4. **Quality Controls**:
   - Token counting to ensure adequate response length
   - Source validation and classification
   - Error handling that maintains context

### **Citation and Mention Detection System**

#### **Citation Detection** (`extract_citations`):
```python
# Detects markdown links: [text](url)
pattern = r'\[([^\]]+)\]\(([^)]+)\)'
```

**Features**:
- **Position Tracking**: Records character positions of citations
- **Context Extraction**: Captures surrounding text for context
- **URL Validation**: Ensures URLs are properly formatted
- **Duplicate Handling**: Prevents counting same citation multiple times

#### **Brand Mention Detection** (`extract_mentions`):
```python
# Detects multiple formats:
# 1. [Brand](url) - markdown links
# 2. Brand[url] - inline links  
# 3. Brand - standalone mentions
```

**Smart Filtering**:
- **URL Exclusion**: Ignores brand mentions within URLs (e.g., "tesla.com")
- **Case Insensitive**: Matches brand names regardless of capitalization
- **Context Awareness**: Distinguishes between different mention types
- **Position Mapping**: Tracks exact locations for analysis

#### **Source Classification** (`classify_sources`):
- **Owned Sources**: URLs matching the brand's domain
- **External Sources**: All other URLs
- **Domain Extraction**: Uses `urlparse` for accurate domain matching
- **Subdomain Handling**: Properly handles subdomains and www prefixes

### **Web Search and Source Budget Strategy**

#### **Budget Constraints**:
```python
CONSTRAINTS
- Max web searches: {max_searches}  # Hard limit on unique domains
- Max sources: {max_sources}       # Hard limit on total sources
```

#### **Search Strategy**:
1. **Domain Diversity**: Each search targets a unique domain
2. **Relevance Focus**: Searches are directed toward answering the specific question
3. **Source Balance**: Encourages mix of owned and external sources
4. **Budget Enforcement**: LLM is explicitly told limits cannot be exceeded

#### **Source Management**:
- **Owned Sources**: Brand's official website and subdomains
- **External Sources**: News, reviews, industry reports, competitor sites
- **Quality Indicators**: Citation context helps assess source reliability
- **Diversity Requirements**: Prevents over-reliance on single sources

#### **Budget Compliance Monitoring**:
- **Real-time Tracking**: Monitors searches and sources during generation
- **Violation Detection**: Identifies when limits are exceeded
- **Graceful Degradation**: Continues analysis even with budget violations
- **Performance Metrics**: Tracks compliance rates for optimization

### **Token Optimization Tactics**

#### **Hierarchical Compression Strategy**:
```python
def create_prompt(brand_name, website_url, question, max_searches, max_sources):
    q_tokens = count_tokens(question)
    
    if q_tokens > question_token_threshold:  # Default: 10 tokens
        final_question = hf_summarize(question, ratio=question_ratio)  # Default: 0.2
    else:
        final_question = question
```

#### **Compression Techniques**:

1. **Question Compression**:
   - **Threshold-Based**: Only compresses questions > 10 tokens
   - **Hugging Face BART**: Uses `facebook/bart-large-cnn` model
   - **Ratio Control**: Maintains 20% of original content
   - **Fallback Safety**: Uses original question if compression fails

2. **Prompt Optimization**:
   - **Constraint Clarity**: Clear, concise constraint statements
   - **Format Specification**: Direct formatting instructions
   - **Essential Information**: Only includes necessary brand details
   - **Token Efficiency**: Optimized for minimal token usage

3. **Response Optimization**:
   - **Citation Efficiency**: Encourages inline citations
   - **Content Density**: Balances detail with conciseness
   - **Markdown Format**: Structured but lightweight formatting

#### **Performance Monitoring**:
- **Token Counting**: Real-time input/output token tracking
- **Efficiency Metrics**: Token reduction percentages
- **Quality Preservation**: Citation and mention counts
- **Budget Compliance**: Search and source limit adherence

#### **Adaptive Optimization**:
- **Question Length**: Different strategies for short vs long questions
- **Content Type**: Adjusts compression based on question complexity
- **Error Handling**: Graceful fallback when optimization fails
- **Learning**: Performance data informs future optimizations

## üöÄ **Performance Optimizations**

### **Token Optimization Results**
Based on comprehensive evaluation of 4 tokenization techniques:

| Technique | Avg Tokens | Avg Duration | Avg Citations | Token Efficiency |
|-----------|------------|--------------|---------------|------------------|
| **Main_Py_Standard** | 801 | 8.51s | 7.5 | 4.451 |
| **BPE_Optimized** | 727 | 8.34s | 5.5 | **9.389** ‚≠ê |
| **Semantic_Compression** | **593** ‚≠ê | **6.87s** ‚≠ê | 3.2 | 8.123 |
| **Hierarchical_Compression** | 765 | 8.10s | 6.5 | 9.066 |

### **Key Improvements:**
- **Semantic Compression**: 26% fewer tokens, 19% faster
- **BPE Optimization**: Highest token efficiency (9.389 ratio)
- **All techniques** show significant improvements over standard prompts

### **Token Efficiency Explained**

**Formula**: `Token Efficiency = Output Tokens / Input Tokens`

- **Higher ratio** = More output per input token (better efficiency)
- **Lower ratio** = Less output per input token (worse efficiency)

**Why This Matters**:
- **Higher efficiency** = Better value for money (more response per token spent)
- **Lower efficiency** = More expensive (more input tokens needed for same response)

**Trade-offs**:
- **Main_Py_Standard**: Lowest efficiency (3.942) but highest quality (7.2 citations)
- **Compressed techniques**: Higher efficiency but potentially lower quality
- **Semantic_Compression**: Best balance - 2.3x better efficiency than main.py while maintaining good quality

## üöÄ **How to Run the Program**

### **1. Setup Environment**
```bash
# Clone and navigate to the project
cd /path/to/brand_analyzer

# Activate virtual environment
source venv/bin/activate

# Install the package (this installs dependencies and creates global command)
cd src
pip install -e .
```

### **2. Configure Environment Variables**
Create a `.env` file in the project root:
```bash
OPENAI_API_KEY=your_api_key_here
MODEL_NAME=nousresearch/hermes-2-pro-llama-3-8b
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
MAX_TOKENS=4000
```

### **3. Main Brand Analysis (CLI)**

**Note**: All methods now use the **optimized prompt** (`create_prompt`) for better token efficiency based on our evaluation results.

#### **Basic Analysis**
```bash
# Method 1: Global command (recommended - works from anywhere)
brand-analyzer analyze --brand "Tesla" --url "https://tesla.com" --question "What are Tesla's latest innovations?"

# Method 2: Root executable (works from project directory)
./brand-analyzer analyze --brand "Tesla" --url "https://tesla.com" --question "What are Tesla's latest innovations?"

# Method 3: Direct Python execution (works from src directory)
cd src
python main.py analyze --brand "Tesla" --url "https://tesla.com" --question "What are Tesla's latest innovations?"

# With custom parameters
brand-analyzer analyze --brand "Google" --url "https://google.com" --question "Tell me about Google's AI initiatives" --max-searches 8 --max-sources 15

# With verbose output
brand-analyzer analyze --brand "Tesla" --url "https://tesla.com" --question "What are Tesla's latest innovations?" --verbose
```

#### **Statistics and Configuration**
```bash
# Show analysis statistics from output/output.json
brand-analyzer stats
# or
./brand-analyzer stats
# or
cd src && python main.py stats

# Show configuration
brand-analyzer config
# or
./brand-analyzer config
# or
cd src && python main.py config

# Get help
brand-analyzer --help
brand-analyzer analyze --help
```

#### **Advanced Usage**
```bash
# Custom output file
brand-analyzer analyze --brand "Tesla" --url "https://tesla.com" --question "What are Tesla's latest innovations?" --output "output/tesla_analysis.json"

# High-resource analysis
brand-analyzer analyze --brand "Google" --url "https://google.com" --question "Comprehensive analysis of Google's AI strategy" --max-searches 10 --max-sources 20 --verbose
```

### **4. Token Optimization Evaluation**

#### **Run Token Optimization Tests**
```bash
# Navigate to evaluation scripts
cd evaluation/scripts

# Run token optimization evaluation
python token_optimization.py

# Generate analysis plots and CSV
python analysis_result.py
```

#### **View Results**
```bash
# Check generated files
ls ../results/
# - token_efficiency_overview.png
# - token_analysis_long_prompts.png  
# - token_analysis_short_prompts.png
# - token_analysis_results.csv
```

### **5. File Structure After Running**

```
brand_analyzer/
‚îú‚îÄ‚îÄ brand-analyzer                       # Root executable (alternative method)
‚îú‚îÄ‚îÄ output/                              # Main analysis outputs
‚îÇ   ‚îî‚îÄ‚îÄ output.json                     # Brand analysis results
‚îú‚îÄ‚îÄ src/                                 # Source code and package
‚îÇ   ‚îú‚îÄ‚îÄ main.py                         # Main CLI application
‚îÇ   ‚îú‚îÄ‚îÄ utils.py                        # Core utilities (optimized prompts)
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt                # Dependencies
‚îÇ   ‚îú‚îÄ‚îÄ setup.py                        # Package installer
‚îÇ   ‚îî‚îÄ‚îÄ brand_analyzer/                 # Python package
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ cli.py
‚îú‚îÄ‚îÄ evaluation/results/                  # Token optimization results
‚îÇ   ‚îú‚îÄ‚îÄ token_efficiency_overview.png   # Overall efficiency analysis
‚îÇ   ‚îú‚îÄ‚îÄ token_analysis_long_prompts.png # Long prompts analysis
‚îÇ   ‚îú‚îÄ‚îÄ token_analysis_short_prompts.png # Short prompts analysis
‚îÇ   ‚îî‚îÄ‚îÄ token_analysis_results.csv      # Token analysis data
‚îú‚îÄ‚îÄ evaluation/scripts/                  # Analysis scripts
‚îÇ   ‚îú‚îÄ‚îÄ token_optimization.py           # Main evaluation script
‚îÇ   ‚îú‚îÄ‚îÄ analysis_result.py              # Visualization script
‚îÇ   ‚îî‚îÄ‚îÄ *.json                          # Data files
‚îî‚îÄ‚îÄ cache/                              # Automatic caching
```

### **Programmatic Usage**
```python
# Add src directory to path
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent / 'src'))

from utils import generate_llm_response, create_prompt, extract_citations, extract_mentions

# Create optimized prompt
prompt = create_prompt("Tesla", "https://tesla.com", "What are Tesla's latest innovations?", 3, 6)

# Generate response
response = generate_llm_response(prompt, "Tesla")

# Extract analysis
citations = extract_citations(response)
mentions = extract_mentions(response, "Tesla")
```

## üöÄ **Installation**

### **Quick Start**
```bash
# Clone the repository
git clone <repository-url>
cd brand_analyzer

# Install dependencies
pip install -r requirements.txt

# Make CLI executable
chmod +x brand-analyzer

# Run the CLI
./brand-analyzer --help
```

### **Install as Package (Optional)**
```bash
# Install in development mode
pip install -e .

# Now you can use brand-analyzer from anywhere
brand-analyzer --help
```

## üîß **Configuration**

### **Environment Variables** (`.env` file):
```bash
OPENAI_API_KEY=your_api_key_here
MODEL_NAME=nousresearch/hermes-2-pro-llama-3-8b
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
MAX_TOKENS=4000
```

### **Dependencies** (`requirements.txt`):
- `click` - CLI interface
- `tiktoken` - Token counting
- `python-dotenv` - Environment management
- `pandas` - Data analysis
- `matplotlib` - Visualizations
- `openai` - LLM API integration

## üìä **Key Features**

### **1. Intelligent Web Search**
- LLM-powered search capabilities
- Automatic source validation
- Context-aware search queries

### **2. Advanced Citation Analysis**
- Markdown link extraction
- Source classification (owned vs. external)
- Position and context tracking

### **3. Brand Mention Detection**
- Multiple mention formats support
- URL-aware filtering
- Linked vs. unlinked mention classification

### **4. Token Optimization**
- BPE optimization
- Semantic compression
- Hierarchical compression
- Performance monitoring

### **5. Comprehensive Evaluation**
- Real-world performance testing
- Quality vs. efficiency analysis
- Detailed metrics collection
- Visualization and reporting

## üéØ **Recommendations**

### **For Production Use**:
1. **Overall Best**: Use **Semantic_Compression** for optimal balance
2. **Speed Priority**: Use **Semantic_Compression** (8.68s avg)
3. **Quality Priority**: Use **Main_Py_Standard** (7.2 citations avg)
4. **Efficiency Priority**: Use **BPE_Optimized** (9.170 ratio)

### **Implementation Strategy**:
1. **Phase 1**: Implement Semantic_Compression as default
2. **Phase 2**: Add quality monitoring to ensure standards
3. **Phase 3**: Implement adaptive optimization based on content type
4. **Phase 4**: Add real-time performance monitoring

## üîç **Technical Details**

### **LLM Configuration**:
- **Model**: `nousresearch/hermes-2-pro-llama-3-8b`
- **Provider**: OpenRouter
- **Max Tokens**: 4000
- **Temperature**: 0.7

### **Token Counting**:
- Uses `tiktoken` with `cl100k_base` encoding
- Fallback to GPT-4 encoding if available
- Accurate token counting for optimization

### **Caching System**:
- Content-based hashing for cache keys
- Automatic cache management
- Performance optimization through caching

## üìà **Future Enhancements**

1. **Adaptive Optimization**: Dynamic technique selection based on content
2. **Quality Metrics**: Real-time quality monitoring
3. **A/B Testing**: Continuous performance evaluation
4. **Multi-Model Support**: Support for different LLM models
5. **Real-Time Monitoring**: Live performance dashboards

## ü§ù **Contributing**

The project is designed for extensibility:
- Add new optimization techniques in `token_optimization.py`
- Extend evaluation metrics in `fresh_tokenization_evaluation.py`
- Enhance utility functions in `utils.py`
- Improve main application in `main.py`

## üìù **Conclusion**

The Brand Analyzer represents a sophisticated approach to brand analysis, combining:
- **Intelligent LLM Integration**: Advanced prompt engineering and response processing
- **Token Optimization**: Multiple techniques for efficient token usage
- **Comprehensive Evaluation**: Rigorous testing and performance analysis
- **Production Ready**: Robust error handling and caching systems

